{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwaVdrCXsHq6LM1WRJggX1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poring3995/Assignment-9-/blob/main/Collaborative_filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt-get update --fix-missing\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "#!wget -q https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "!mv spark-3.0.0-bin-hadoop3.2.tgz sparkkk\n",
        "!tar xf sparkkk\n",
        "!pip install -q findspark\n",
        "     \n",
        "\n",
        "#pip install spark\n",
        "     \n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('fpgrowth') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark   "
      ],
      "metadata": {
        "id": "qXCdIMGBPtM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py0U1JC4Ovn7",
        "outputId": "6afd5bc9-1ba7-4b3d-b14d-4f06dae52ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root-mean-square error = 1.2274912663121302\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "\n",
        "lines = spark.read.text(\"/content/spark-3.0.0-bin-hadoop3.2/data/sample_movielens_ratings.txt\").rdd\n",
        "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
        "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
        "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
        "ratings = spark.createDataFrame(ratingsRDD)\n",
        "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Build the recommendation model using ALS on the training data\n",
        "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
        "als = ALS(maxIter=10, regParam=0.5, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
        "          coldStartStrategy=\"drop\")\n",
        "model = als.fit(training)\n",
        "\n",
        "# Evaluate the model by computing the RMSE on the test data\n",
        "predictions = model.transform(test)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "                                predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root-mean-square error = \" + str(rmse))\n",
        "\n",
        "# Generate top 10 movie recommendations for each user\n",
        "userRecs = model.recommendForAllUsers(10)\n",
        "# Generate top 10 user recommendations for each movie\n",
        "movieRecs = model.recommendForAllItems(10)\n",
        "\n",
        "# Generate top 10 movie recommendations for a specified set of users\n",
        "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
        "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
        "# Generate top 10 user recommendations for a specified set of movies\n",
        "movies = ratings.select(als.getItemCol()).distinct().limit(3)\n",
        "movieSubSetRecs = model.recommendForItemSubset(movies, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion :** \n",
        "\n",
        "for the model after I do some experimental with with several hyperparameters values (maxIter and regParam) as I try it with 4 cases model\n",
        "\n",
        "1st model is using maxIter = 5. regParam = 0.1\n",
        "\n",
        "Average RMSE (based on running for 10 times) = 1.1736360154494294\n",
        "\n",
        "2nd model is using maxIter = 10, regParam = 0.1\n",
        "\n",
        "Average RMSE (based on running for 10 times) = 1.0568265521064983\n",
        "\n",
        "3rd model is using maxIter = 5, regParam = 0.5\n",
        "\n",
        "Average RMSE (based on running for 10 times) = 1.2015760364620547\n",
        "\n",
        "4th model is using maxIter = 10, regParam = 0.5\n",
        "\n",
        "Average RMSE (based on running for 10 times) = 1.1851709836369329\n",
        "\n",
        "\n",
        "So, base on the experimental I would say that the 2nd model is the best performance among these 4 models since it has the less RMSE, according to the changing the value of the hyper parameter with base on these 4 models experimental, we can see that the more number of iteration loop would provide more accuracy result which may due to the factor that the model has more step to be improved and prediction, but for the regularizer parameter we can see that it quite opposite to the Iteration loop since the more more value of it would causes the lower accuracy to the model, since it would aim to reduce the overfitting of the model and with too much regularizer rate it could make the model much less effective which could be decrease the model accuracy.\n"
      ],
      "metadata": {
        "id": "H6WpWifoZzae"
      }
    }
  ]
}